{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# SIPI-ETL: Pipeline de DetecciÃ³n\n",
    "\n",
    "Este notebook te permite probar el pipeline ETL paso a paso:\n",
    "1. **Setup**: Configurar entorno y conexiones\n",
    "2. **Registry**: Verificar que los scrapers estÃ¡n registrados\n",
    "3. **ConexiÃ³n BD**: Verificar PostgreSQL y Redis\n",
    "4. **Scraper**: Probar scraping sin guardar\n",
    "5. **Scoring**: Evaluar inmuebles en memoria\n",
    "6. **Pipeline completo**: Scraping + Scoring + Guardado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-header",
   "metadata": {},
   "source": [
    "## 1. Setup Inicial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "import pandas as pd\n",
    "\n",
    "# Permitir async en Jupyter\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# AÃ±adir path del proyecto\n",
    "project_root = Path.cwd().parent if (Path.cwd() / 'notebooks').exists() else Path.cwd()\n",
    "if (project_root / 'notebooks').exists():\n",
    "    project_root = project_root  # Ya estamos en la raÃ­z\n",
    "elif (project_root.parent / 'src').exists():\n",
    "    project_root = project_root.parent  # Estamos en notebooks/\n",
    "\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "print(f\"âœ“ Project root: {project_root}\")\n",
    "print(f\"âœ“ Python version: {sys.version.split()[0]}\")\n",
    "\n",
    "# Configurar variables de entorno para PostgreSQL\n",
    "os.environ['POSTGRES_HOST'] = 'postgis'\n",
    "os.environ['POSTGRES_PORT'] = '5432'\n",
    "os.environ['POSTGRES_DB'] = 'spatialdb'\n",
    "os.environ['POSTGRES_USER'] = 'user'\n",
    "os.environ['POSTGRES_PASSWORD'] = 'password'\n",
    "os.environ['REDIS_HOST'] = 'redis'\n",
    "\n",
    "print(\"âœ“ Variables de entorno configuradas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config-header",
   "metadata": {},
   "source": [
    "## 2. ConfiguraciÃ³n del Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.core.etl_event_system import PortalType\n",
    "\n",
    "# ParÃ¡metros del pipeline\n",
    "CONFIG = {\n",
    "    'portal': PortalType.IDEALISTA,\n",
    "    'provincia': 'sevilla',\n",
    "    'max_pages': 1,  # Solo 1 pÃ¡gina para testing rÃ¡pido\n",
    "    'max_items': 10,  # MÃ¡ximo de inmuebles a procesar\n",
    "    'batch_size': 50,\n",
    "    'enable_dedup': True,\n",
    "    'enable_screenshots': False,  # Deshabilitado por defecto para testing rÃ¡pido\n",
    "    'threshold': 50.0  # Score mÃ­nimo para considerar una detecciÃ³n\n",
    "}\n",
    "\n",
    "print(\"ConfiguraciÃ³n del pipeline:\")\n",
    "for k, v in CONFIG.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "registry-header",
   "metadata": {},
   "source": [
    "## 3. Verificar Registry de Scrapers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "registry",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.modules.portals.factory import _SCRAPER_REGISTRY, create_scraper, get_available_portals\n",
    "\n",
    "# Verificar scrapers registrados\n",
    "portals = get_available_portals()\n",
    "portal_names = [p.value for p in portals]\n",
    "\n",
    "print(f\"Scrapers disponibles: {portal_names}\")\n",
    "\n",
    "if CONFIG['portal'] in _SCRAPER_REGISTRY:\n",
    "    print(f\"âœ“ Scraper para '{CONFIG['portal'].value}' encontrado\")\n",
    "    \n",
    "    # Crear instancia del scraper\n",
    "    scraper = create_scraper(CONFIG['portal'])\n",
    "    print(f\"âœ“ Scraper creado: {scraper.__class__.__name__}\")\n",
    "    print(f\"  Base URL: {scraper.base_url}\")\n",
    "    \n",
    "    # Test URL generation\n",
    "    test_url = scraper.get_search_url(provincia='sevilla', pagina=1)\n",
    "    print(f\"  URL de prueba: {test_url}\")\n",
    "else:\n",
    "    print(f\"âŒ Scraper para '{CONFIG['portal'].value}' NO encontrado\")\n",
    "    print(\"   SoluciÃ³n: Verificar que __init__.py importa correctamente el scraper\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db-header",
   "metadata": {},
   "source": [
    "## 4. Verificar Conexiones a Base de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-postgres",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.modules.portals.base_loader import PostgresConnectionPool\n",
    "\n",
    "async def test_postgres():\n",
    "    \"\"\"Verificar conexiÃ³n a PostgreSQL\"\"\"\n",
    "    try:\n",
    "        pool = await PostgresConnectionPool.get_pool()\n",
    "        \n",
    "        async with pool.acquire() as conn:\n",
    "            # Verificar versiÃ³n\n",
    "            version = await conn.fetchval('SELECT version()')\n",
    "            print(f\"âœ“ PostgreSQL conectado\")\n",
    "            print(f\"  VersiÃ³n: {version.split(',')[0]}\")\n",
    "            \n",
    "            # Verificar tablas\n",
    "            tables = ['inmuebles_raw', 'detecciones', 'cambios', 'duplicates']\n",
    "            \n",
    "            print(\"\\n  Tablas en portals:\")\n",
    "            for table in tables:\n",
    "                exists = await conn.fetchval(\n",
    "                    \"SELECT EXISTS(SELECT 1 FROM information_schema.tables \"\n",
    "                    \"WHERE table_schema = 'portals' AND table_name = $1)\",\n",
    "                    table\n",
    "                )\n",
    "                \n",
    "                if exists:\n",
    "                    count = await conn.fetchval(f\"SELECT COUNT(*) FROM portals.{table}\")\n",
    "                    print(f\"    âœ“ {table}: {count} registros\")\n",
    "                else:\n",
    "                    print(f\"    âœ— {table}: NO EXISTE\")\n",
    "        \n",
    "        await PostgresConnectionPool.close_pool()\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error PostgreSQL: {e}\")\n",
    "        print(\"\\n   SoluciÃ³n: Ejecutar init_db.py para crear las tablas\")\n",
    "        return False\n",
    "\n",
    "# Ejecutar test\n",
    "postgres_ok = await test_postgres()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-redis",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.modules.portals.redis_cache import RedisCache\n",
    "\n",
    "async def test_redis():\n",
    "    \"\"\"Verificar conexiÃ³n a Redis\"\"\"\n",
    "    try:\n",
    "        cache = RedisCache()\n",
    "        await cache.connect()\n",
    "        \n",
    "        # Test bÃ¡sico\n",
    "        test_key = 'notebook_test'\n",
    "        is_dup_1 = await cache.check_duplicate('test', test_key, ttl_hours=1)\n",
    "        is_dup_2 = await cache.check_duplicate('test', test_key, ttl_hours=1)\n",
    "        \n",
    "        await cache.close()\n",
    "        \n",
    "        if not is_dup_1 and is_dup_2:\n",
    "            print(\"âœ“ Redis conectado y funcional\")\n",
    "            print(\"  DeduplicaciÃ³n: OK\")\n",
    "            return True\n",
    "        else:\n",
    "            print(\"âš  Redis conecta pero deduplicaciÃ³n no funciona\")\n",
    "            return False\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âš  Redis no disponible: {str(e)[:100]}\")\n",
    "        print(\"  (Opcional - el sistema puede funcionar sin Redis)\")\n",
    "        return False\n",
    "\n",
    "# Ejecutar test\n",
    "redis_ok = await test_redis()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scraper-header",
   "metadata": {},
   "source": [
    "## 5. Test del Scraper (sin guardar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-scraper",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTA: Este test requiere que el scraper real de Idealista estÃ© implementado\n",
    "# Si el scraper usa Selenium y no estÃ¡ completamente implementado, puede fallar\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TEST DEL SCRAPER\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nNOTA: Este test intentarÃ¡ scrapear inmuebles reales de Idealista.\")\n",
    "print(\"Si el scraper no estÃ¡ completamente implementado, puede fallar.\")\n",
    "print(\"\\nPara un test completo del pipeline, salta a la secciÃ³n 6.\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# Descomentar para ejecutar:\n",
    "# async def quick_scraper_test():\n",
    "#     \"\"\"Test rÃ¡pido del scraper\"\"\"\n",
    "#     scraper = create_scraper(CONFIG['portal'])\n",
    "#     \n",
    "#     print(f\"Scraping {CONFIG['provincia']} (mÃ¡x {CONFIG['max_items']} inmuebles)...\")\n",
    "#     \n",
    "#     # AquÃ­ irÃ­a el cÃ³digo de scraping real\n",
    "#     # Por ahora solo mostramos que el scraper se puede crear\n",
    "#     print(f\"âœ“ Scraper listo: {scraper.__class__.__name__}\")\n",
    "#     \n",
    "# await quick_scraper_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary-header",
   "metadata": {},
   "source": [
    "## 6. Resumen de Verificaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RESUMEN DE VERIFICACIONES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Registry\n",
    "registry_ok = len(portal_names) > 0 and CONFIG['portal'] in _SCRAPER_REGISTRY\n",
    "print(f\"{'âœ“' if registry_ok else 'âœ—'} Registry de scrapers\")\n",
    "if registry_ok:\n",
    "    print(f\"    Scrapers disponibles: {portal_names}\")\n",
    "else:\n",
    "    print(\"    âš  Registry vacÃ­o - verificar __init__.py\")\n",
    "\n",
    "# PostgreSQL\n",
    "print(f\"\\n{'âœ“' if postgres_ok else 'âœ—'} ConexiÃ³n PostgreSQL\")\n",
    "if not postgres_ok:\n",
    "    print(\"    âš  Ejecutar: init_db.py\")\n",
    "\n",
    "# Redis\n",
    "print(f\"\\n{'âœ“' if redis_ok else 'âš '} ConexiÃ³n Redis (opcional)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "if registry_ok and postgres_ok:\n",
    "    print(\"\\nðŸŽ‰ Sistema listo para usar\")\n",
    "    print(\"\\nPrÃ³ximos pasos:\")\n",
    "    print(\"  1. Implementar el scraping real (si no estÃ¡ hecho)\")\n",
    "    print(\"  2. Probar el scoring de inmuebles\")\n",
    "    print(\"  3. Ejecutar el pipeline completo\")\n",
    "else:\n",
    "    print(\"\\nâš  Hay problemas que resolver\")\n",
    "    if not registry_ok:\n",
    "        print(\"\\n  Registry:\")\n",
    "        print(\"    - Verificar que __init__.py importa los scrapers\")\n",
    "        print(\"    - Reiniciar el kernel de Jupyter\")\n",
    "    if not postgres_ok:\n",
    "        print(\"\\n  PostgreSQL:\")\n",
    "        print(\"    - Ejecutar: exec(open('/home/jovyan/dev/sipi-etl/init_db.py').read())\")\n",
    "\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "notes-header",
   "metadata": {},
   "source": [
    "## Notas Importantes\n",
    "\n",
    "### Variables de Entorno\n",
    "Este notebook configura automÃ¡ticamente las variables de entorno para conectar con los servicios Docker:\n",
    "- `POSTGRES_HOST=postgis` (nombre del servicio Docker)\n",
    "- `REDIS_HOST=redis` (nombre del servicio Docker)\n",
    "\n",
    "### Estructura del Proyecto\n",
    "```\n",
    "sipi-etl/\n",
    "â”œâ”€â”€ src/\n",
    "â”‚   â””â”€â”€ modules/\n",
    "â”‚       â””â”€â”€ portals/\n",
    "â”‚           â”œâ”€â”€ __init__.py          (registra scrapers)\n",
    "â”‚           â”œâ”€â”€ base_loader.py       (conexiÃ³n PostgreSQL)\n",
    "â”‚           â”œâ”€â”€ redis_cache.py       (conexiÃ³n Redis)\n",
    "â”‚           â””â”€â”€ idealista/\n",
    "â”‚               â”œâ”€â”€ __init__.py      (expone IdealistaScraper)\n",
    "â”‚               â””â”€â”€ extract/\n",
    "â”‚                   â””â”€â”€ scraper.py   (@register_scraper)\n",
    "â””â”€â”€ notebooks/\n",
    "    â””â”€â”€ test_pipeline.ipynb          (este notebook)\n",
    "```\n",
    "\n",
    "### Troubleshooting\n",
    "\n",
    "#### Registry vacÃ­o\n",
    "1. Verificar que todos los `__init__.py` existen\n",
    "2. **Reiniciar el kernel** (Kernel â†’ Restart Kernel)\n",
    "3. Volver a ejecutar las celdas desde el inicio\n",
    "\n",
    "#### Error de conexiÃ³n PostgreSQL\n",
    "1. Verificar que el contenedor `postgis` estÃ¡ corriendo: `docker ps`\n",
    "2. Ejecutar `init_db.py` para crear las tablas\n",
    "3. Verificar que las credenciales son correctas\n",
    "\n",
    "#### Imports fallan\n",
    "1. Verificar que estÃ¡s en el directorio correcto\n",
    "2. Verificar que `project_root` apunta a la raÃ­z del proyecto\n",
    "3. Verificar que `src/` existe en `project_root`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
